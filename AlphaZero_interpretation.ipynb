{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AlphaZero interpretation",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vochicong/ai-memo/blob/master/AlphaZero_interpretation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "sHPkIc8v2YBg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# AlphaZero interpretion\n",
        "> *Draft, Apr. 5th, 2019*\n",
        "\n",
        "What exactly the Alphas do?\n",
        "Why is AlphaZero stronger than AlphaGo Zero?\n",
        "Can we use/enjoy Alpha algorithms on a general CPU/GPU\n",
        "or even a mobile device?\n",
        "\n",
        "Alpha variants\n",
        "- AlphaGo, 2016\n",
        "- AlphaGo Zero, 2017a\n",
        "- AlphaZero, 2017b\n",
        "\n",
        "AlphaGo Zero is stronger than AlphaGo, since it has discovered some superhuman strategy for playing Go through self-plays. However it is not clear whether AlphaZero is ultimately stronger than AlphaGo Zero.\n",
        "\n",
        "## AlphaGo\n",
        "\n",
        "Training data for SL (supervised learning) is a collection of nearly 30 milion human expert moves.\n",
        "\n",
        "\"Asynchronouse policy and value MCTS\", or APV-MCTS expands its tree by choosing an action according to probabilities supplied by a 13-layer deep convolutional ANN, called the *SL policy network*.\n",
        "\n",
        "Value of a newly-added node $s$ is a mixing \n",
        "$$ v(s) = (1-\\eta)v_\\theta(s) + \\eta G$$\n",
        "where\n",
        "$v_\\theta$ is a value function learned by a RL method, and\n",
        "$G$ is the return of the **rollout** from state $s$.\n",
        "\n",
        "The **rollout** policy is a fast, simple linear network trained by SL.\n",
        "\n",
        "![AlphaGo pipeline](https://www.researchgate.net/profile/Daniele_Grattarola/publication/323218981/figure/fig15/AS:594583629090816@1518771192934/Neural-network-training-pipeline-of-AlphaGo-image-taken-from-39.png)\n",
        "\n",
        "The *RL policy network* has the same structure as the SL policy network. It is initialized with the final weights of the SL policy network, and improved through policy-gradient reinforcement learning.\n",
        "\n",
        "The *value network* has the same structure as the SL (and RL) policy network, and is trained by Monte Carlo policy evaluation on a large number of self-played games with moves selected by the RL policy network.\n",
        "\n",
        "\n",
        "## AlphaZero\n",
        "\n",
        "AlphaZero is simpler and more general then AlphaGo Zero.\n",
        "\n",
        "The NN (neural network) $f_\\theta$ starts from randomly initialized parameters $\\theta$. It evaluates a specific game position $s$,\n",
        "$$ ({\\bf p}, v) = f_\\theta(s), $$\n",
        "where ${\\bf p}$ is the policy vector and $v$ is the predicted outcome.\n",
        "\n",
        "An MCTS (Monte Carlo Tree Search) upon a position\n",
        "$s_{root} = s_t$ at turn $t$ selects a move \n",
        "$a_t \\sim {\\bf \\pi_t}$\n",
        "where the search probabilities $\\bf \\pi_t$ relates to the visit counts at the state.\n",
        "\n",
        "Loss function is a sum over a set of self-play games,\n",
        "$$ l_\\theta = \\sum\\left\\{\n",
        "(z-v)^2 - {\\bf \\pi}^\\top\\log{\\bf p}\n",
        "\\right\\}\n",
        "+c||\\theta||^2$$\n",
        "where $c$ is a $L_2$ weight regularization parameter.\n",
        "\n",
        "## AlphaGo Zero\n",
        "\n",
        "\n",
        "---\n",
        "## Refrences\n",
        "\n",
        "- Sutton & Barto Book: Reinforcement Learning: An Introduction, 2nd ed.\n",
        "- AlphaGo, AlphaGo Zero and AlphaZero related papers by DeepMind\n"
      ]
    },
    {
      "metadata": {
        "id": "ZMMucxRSI0Lg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}