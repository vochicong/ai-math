{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # TF Agent で Blackjack 遊ぶ\n",
    "\n",
    " ブラックジャックを参考に、次のような\n",
    " ルールとする。\n",
    "\n",
    " - カードの値は、 1〜11の間にランダムに決まる（エース考慮などはしない）\n",
    " - 最初にカードをプレイヤーに2枚、ディーラーに1枚\n",
    " - プレイヤーが何枚でもカードを引ける(hit)が、合計が21超えたら即負け。ゲーム終了\n",
    " - プレイヤーがカードを引くのを止めたら(stick)、ディーラーがカードを引く番になる\n",
    " - ディーラーは、カードの合計が17に達するまでカードを強制的に引く\n",
    " - ディーラーのカードの合計が21超えたら、プレイヤーの勝ち。ゲーム終了\n",
    " - ディーラーとプレイヤーとでカードの合計を比較して、高いほうが勝ち。ゲーム終了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "# !which python\n",
    "# !sudo apt install -y cuda-cublas-10-0  cuda-cusolver-10-0 cuda-cudart-10-0 cuda-cusparse-10-0\n",
    "# !conda install -y -c anaconda cudatoolkit\n",
    "# !pip install tf-nightly-gpu tf-agents-nightly 'gym==0.10.11'\n",
    "from tf_agents.environments import utils\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tf_agents.environments import py_environment\n",
    "from tf_agents.environments import tf_environment\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.environments import suite_gym\n",
    "from tf_agents.environments import time_step\n",
    "from tf_agents.specs import array_spec\n",
    "\n",
    "tf.compat.v1.enable_v2_behavior()\n",
    "\n",
    "\n",
    "class BlackJackEnv(py_environment.PyEnvironment):\n",
    "    # Simplified Blackjack\n",
    "    ACT_HIT = 0\n",
    "    ACT_STICK = 1\n",
    "    LIMIT_SCORE = 21\n",
    "    STATE_LEN = 3\n",
    "\n",
    "    def __init__(self):\n",
    "        self._action_spec = array_spec.BoundedArraySpec(\n",
    "            shape=(), dtype=np.int32, name='action',\n",
    "            minimum=self.ACT_HIT, maximum=self.ACT_STICK,\n",
    "        )\n",
    "        self._observation_spec = array_spec.BoundedArraySpec(\n",
    "            shape=(self.STATE_LEN,), dtype=np.int32, minimum=0,\n",
    "            name='observation'\n",
    "        )\n",
    "        self._reset()\n",
    "        return\n",
    "\n",
    "    def _state(self):\n",
    "        # Full state includes 1st card of the dealer and all cards of player,\n",
    "        # but this return only the last STATE_LEN cards.\n",
    "        state = [self._dealer_cards[0]] + self._player_cards\n",
    "        return np.array(state[-self.STATE_LEN:], dtype=np.int32)\n",
    "\n",
    "    def action_spec(self):\n",
    "        return self._action_spec\n",
    "\n",
    "    def observation_spec(self):\n",
    "        return self._observation_spec\n",
    "\n",
    "    def _reset(self):\n",
    "        self._player_cards = [self._new_card(), self._new_card()]\n",
    "        self._dealer_cards = [self._new_card()]\n",
    "        return time_step.restart(self._state())\n",
    "\n",
    "    def _new_card(self):\n",
    "        # Simplified Blackjack rule\n",
    "        new_card = np.random.randint(1, 11+1)\n",
    "        return new_card\n",
    "\n",
    "    def _dealer_hit(self):\n",
    "        while np.sum(self._dealer_cards) < 17:\n",
    "            self._dealer_cards.append(self._new_card())\n",
    "        return np.sum(self._dealer_cards)\n",
    "\n",
    "    def _player_score(self):\n",
    "        return np.sum(self._player_cards)\n",
    "\n",
    "    def _step(self, action):\n",
    "        if action == self.ACT_HIT:\n",
    "            self._player_cards.append(self._new_card())\n",
    "            if self._player_score() > self.LIMIT_SCORE:  # the player goes bust\n",
    "                return time_step.termination(self._state(), -1)\n",
    "\n",
    "            return time_step.transition(self._state(), reward=0, discount=1)\n",
    "\n",
    "        # Afteward action == self.ACT_STICK\n",
    "        dealer_score = self._dealer_hit()\n",
    "        player_score = self._player_score()\n",
    "        if dealer_score > self.LIMIT_SCORE or dealer_score < player_score:\n",
    "            reward = 1\n",
    "        elif dealer_score == player_score:\n",
    "            reward = 0\n",
    "        else:\n",
    "            reward = -1\n",
    "        return time_step.termination(self._state(), reward)\n",
    "\n",
    "\n",
    "def print_spec(env):\n",
    "    act_spec, ts_spec = env.action_spec(), env.time_step_spec()\n",
    "    for x in (act_spec, ts_spec.observation, ts_spec.step_type,\n",
    "              ts_spec.discount, ts_spec.reward):\n",
    "        print(x)\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## ランダムに遊ぶ場合\n",
    "\n",
    " プレイヤーがカードを最大 `n_max_cards` 枚引く。\n",
    " 平均的に見たら負けています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0409 17:35:06.483657 140586977675072 backprop.py:818] The dtype of the watched tensor must be floating (e.g. tf.float32), got tf.int32\n",
      "W0409 17:35:06.487196 140586977675072 backprop.py:818] The dtype of the watched tensor must be floating (e.g. tf.float32), got tf.int32\n",
      "W0409 17:35:06.490557 140586977675072 backprop.py:818] The dtype of the watched tensor must be floating (e.g. tf.float32), got tf.int32\n",
      "W0409 17:35:06.494595 140586977675072 backprop.py:818] The dtype of the watched tensor must be floating (e.g. tf.float32), got tf.int32\n",
      "W0409 17:35:06.500438 140586977675072 backprop.py:818] The dtype of the watched tensor must be floating (e.g. tf.float32), got tf.int32\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-0.337"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def play_blackjack(env, n_max_cards=1):\n",
    "    ts = env.reset()\n",
    "    gain = ts.reward\n",
    "    cards = []\n",
    "    for _ in range(np.random.randint(n_max_cards+1)):\n",
    "        if ts.is_last():\n",
    "            break\n",
    "        ts = env.step(tf.constant([BlackJackEnv.ACT_HIT]))\n",
    "        cards += [ts.observation[0][0].numpy()]\n",
    "        gain += ts.reward\n",
    "\n",
    "    if not ts.is_last():\n",
    "        ts = env.step(tf.constant([BlackJackEnv.ACT_STICK]))\n",
    "        gain += ts.reward\n",
    "    gain = gain.numpy()[0]\n",
    "    return cards, gain\n",
    "\n",
    "\n",
    "utils.validate_py_environment(BlackJackEnv())\n",
    "\n",
    "env = tf_py_environment.TFPyEnvironment(BlackJackEnv())\n",
    "gains = []\n",
    "for _ in range(1000):\n",
    "    _, gain = play_blackjack(env, 2)\n",
    "    gains.append(gain)\n",
    "np.mean(gains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
