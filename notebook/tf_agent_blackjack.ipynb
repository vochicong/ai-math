{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # TF Agent で Blackjack 遊ぶ\n",
    "\n",
    " ブラックジャックを参考に、次のような\n",
    " ルールとする。\n",
    "\n",
    " - カードの値は、 1〜11の間にランダムに決まる（エース考慮などはしない）\n",
    " - 最初にカードをプレイヤーに2枚、ディーラーに1枚\n",
    " - プレイヤーが何枚でもカードを引ける(hit)が、合計が21超えたら即負け。ゲーム終了\n",
    " - プレイヤーがカードを引くのを止めたら(stick)、ディーラーがカードを引く番になる\n",
    " - ディーラーは、カードの合計が17に達するまでカードを強制的に引く\n",
    " - ディーラーのカードの合計が21超えたら、プレイヤーの勝ち。ゲーム終了\n",
    " - ディーラーとプレイヤーとでカードの合計を比較して、高いほうが勝ち。ゲーム終了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "from tf_agents.drivers import dynamic_episode_driver\n",
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.policies import random_tf_policy\n",
    "# !which python\n",
    "# !sudo apt install -y cuda-cublas-10-0  cuda-cusolver-10-0 cuda-cudart-10-0 cuda-cusparse-10-0\n",
    "# !conda install -y -c anaconda cudatoolkit\n",
    "# !pip install tf-nightly-gpu tf-agents-nightly 'gym==0.10.11'\n",
    "from tf_agents.environments import utils\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tf_agents.environments import py_environment\n",
    "from tf_agents.environments import tf_environment\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.environments import suite_gym\n",
    "from tf_agents.environments import time_step\n",
    "from tf_agents.specs import array_spec\n",
    "\n",
    "tf.compat.v1.enable_v2_behavior()\n",
    "\n",
    "\n",
    "class BlackJackEnv(py_environment.PyEnvironment):\n",
    "    # Simplified Blackjack\n",
    "    ACT_HIT = 0\n",
    "    ACT_STICK = 1\n",
    "    LIMIT_SCORE = 21\n",
    "    STATE_LEN = 3\n",
    "\n",
    "    def __init__(self):\n",
    "        self._action_spec = array_spec.BoundedArraySpec(\n",
    "            shape=(), dtype=np.int32, name='action',\n",
    "            minimum=self.ACT_HIT, maximum=self.ACT_STICK,\n",
    "        )\n",
    "        self._observation_spec = array_spec.BoundedArraySpec(\n",
    "            shape=(self.STATE_LEN,), dtype=np.int32, minimum=0,\n",
    "            name='observation'\n",
    "        )\n",
    "        self.__reset()\n",
    "        return\n",
    "\n",
    "    def _state(self):\n",
    "        # Full state includes 1st card of the dealer and all cards of player,\n",
    "        # but this return only the last STATE_LEN cards.\n",
    "        state = [self._dealer_cards[0]] + self._player_cards\n",
    "        return np.array(state[-self.STATE_LEN:], dtype=np.int32)\n",
    "\n",
    "    def action_spec(self):\n",
    "        return self._action_spec\n",
    "\n",
    "    def observation_spec(self):\n",
    "        return self._observation_spec\n",
    "\n",
    "    def __reset(self):\n",
    "        self._player_cards = [self._new_card(), self._new_card()]\n",
    "        self._dealer_cards = [self._new_card()]\n",
    "        self._episode_ended = False\n",
    "\n",
    "    def _reset(self):\n",
    "        self.__reset()\n",
    "        return time_step.restart(self._state())\n",
    "\n",
    "    def _new_card(self):\n",
    "        # Simplified Blackjack rule\n",
    "        new_card = np.random.randint(1, 11+1)\n",
    "        return new_card\n",
    "\n",
    "    def _dealer_hit(self):\n",
    "        while np.sum(self._dealer_cards) < 17:\n",
    "            self._dealer_cards.append(self._new_card())\n",
    "        return np.sum(self._dealer_cards)\n",
    "\n",
    "    def _player_score(self):\n",
    "        return np.sum(self._player_cards)\n",
    "\n",
    "    def _terminate(self, reward):\n",
    "        print(\"Player: {} -> {}. Dealer: {} -> {}. Reward: {}.\".format(\n",
    "            self._player_cards, np.sum(self._player_cards),\n",
    "            self._dealer_cards, np.sum(self._dealer_cards),\n",
    "            reward))\n",
    "        self._episode_ended = True\n",
    "        return time_step.termination(self._state(), reward)\n",
    "\n",
    "    def _step(self, action):\n",
    "        if self._episode_ended:\n",
    "            return self.reset() # don't forget to `return`\n",
    "\n",
    "        if action == self.ACT_HIT:\n",
    "            self._player_cards.append(self._new_card())\n",
    "            if self._player_score() > self.LIMIT_SCORE:  # the player goes bust\n",
    "                return self._terminate(-1)\n",
    "\n",
    "            return time_step.transition(self._state(), reward=0, discount=1)\n",
    "\n",
    "        # Afteward action == self.ACT_STICK\n",
    "        dealer_score = self._dealer_hit()\n",
    "        player_score = self._player_score()\n",
    "        if dealer_score > self.LIMIT_SCORE or dealer_score < player_score:\n",
    "            reward = 1\n",
    "        elif dealer_score == player_score:\n",
    "            reward = 0\n",
    "        else:\n",
    "            reward = -1\n",
    "        return self._terminate(reward)\n",
    "\n",
    "\n",
    "def print_spec(env):\n",
    "    act_spec, ts_spec = env.action_spec(), env.time_step_spec()\n",
    "    for x in (act_spec, ts_spec.observation, ts_spec.step_type,\n",
    "              ts_spec.discount, ts_spec.reward):\n",
    "        print(x)\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## ランダムに遊ぶ場合\n",
    "\n",
    " プレイヤーがカードを最大 `n_max_cards` 枚引く。\n",
    " 平均的に見たら負けています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0410 08:50:18.089698 4457072064 backprop.py:818] The dtype of the watched tensor must be floating (e.g. tf.float32), got tf.int32\n",
      "W0410 08:50:18.095741 4457072064 backprop.py:818] The dtype of the watched tensor must be floating (e.g. tf.float32), got tf.int32\n",
      "W0410 08:50:18.099249 4457072064 backprop.py:818] The dtype of the watched tensor must be floating (e.g. tf.float32), got tf.int32\n",
      "W0410 08:50:18.105082 4457072064 backprop.py:818] The dtype of the watched tensor must be floating (e.g. tf.float32), got tf.int32\n",
      "W0410 08:50:18.108670 4457072064 backprop.py:818] The dtype of the watched tensor must be floating (e.g. tf.float32), got tf.int32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Player: [2, 5] -> 7. Dealer: [5, 7, 2, 11] -> 25. Reward: 1.\n",
      "Player: [11, 3, 3] -> 17. Dealer: [4, 5, 3, 3, 4] -> 19. Reward: -1.\n",
      "Player: [5, 10, 1] -> 16. Dealer: [9, 2, 5, 8] -> 24. Reward: 1.\n",
      "Player: [7, 4, 10, 6] -> 27. Dealer: [1] -> 1. Reward: -1.\n",
      "Player: [5, 4] -> 9. Dealer: [1, 1, 8, 6, 11] -> 27. Reward: 1.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.2"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "def play_blackjack(env, n_max_cards=1):\n",
    "    ts = env.reset()\n",
    "    gain = ts.reward\n",
    "    cards = []\n",
    "    for _ in range(np.random.randint(n_max_cards+1)):\n",
    "        if ts.is_last():\n",
    "            break\n",
    "        ts = env.step(tf.constant([BlackJackEnv.ACT_HIT]))\n",
    "        cards += [ts.observation[0][0].numpy()]\n",
    "        gain += ts.reward\n",
    "\n",
    "    if not ts.is_last():\n",
    "        ts = env.step(tf.constant([BlackJackEnv.ACT_STICK]))\n",
    "        gain += ts.reward\n",
    "    gain = gain.numpy()[0]\n",
    "    return cards, gain\n",
    "\n",
    "\n",
    "# TODO: validate_py_environment should check for a reset()\n",
    "# utils.validate_py_environment(BlackJackEnv())\n",
    "\n",
    "env = tf_py_environment.TFPyEnvironment(BlackJackEnv())\n",
    "gains = []\n",
    "num_eval_episodes = 5  # @param\n",
    "for _ in range(num_eval_episodes):\n",
    "    _, gain = play_blackjack(env, 2)\n",
    "    gains.append(gain)\n",
    "mean_score1 = np.mean(gains)\n",
    "mean_score1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Player: [5, 4] -> 9. Dealer: [2, 6, 8, 2] -> 18. Reward: -1.\n",
      "Player: [10, 8, 9] -> 27. Dealer: [5] -> 5. Reward: -1.\n",
      "Player: [3, 6] -> 9. Dealer: [10, 10] -> 20. Reward: -1.\n",
      "Player: [6, 6] -> 12. Dealer: [9, 6, 9] -> 24. Reward: 1.\n",
      "Player: [4, 3, 1, 10] -> 18. Dealer: [2, 2, 1, 2, 5, 10] -> 22. Reward: 1.\n",
      "Final step TimeStep(step_type=<tf.Tensor: id=1028, shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, reward=<tf.Tensor: id=1029, shape=(1,), dtype=float32, numpy=array([0.], dtype=float32)>, discount=<tf.Tensor: id=1030, shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>, observation=<tf.Tensor: id=1031, shape=(1, 3), dtype=int32, numpy=array([[10,  6,  7]], dtype=int32)>)\n",
      "Number of Steps:  7\n",
      "Number of Episodes:  5\n",
      "Average Return:  -0.2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "num_eval_episodes = 5  # @param\n",
    "\n",
    "\n",
    "def evaluate_policy(\n",
    "        policy,\n",
    "        observers,\n",
    "        eval_env=tf_py_environment.TFPyEnvironment(BlackJackEnv()),\n",
    "        num_episodes=num_eval_episodes):\n",
    "    driver = dynamic_episode_driver.DynamicEpisodeDriver(\n",
    "        eval_env, policy, observers, num_episodes)\n",
    "    final_step, policy_state = driver.run(num_episodes=num_episodes)\n",
    "    print('Final step', final_step)\n",
    "    return driver, final_step, policy_state\n",
    "\n",
    "\n",
    "env = tf_py_environment.TFPyEnvironment(BlackJackEnv())\n",
    "rand_policy = random_tf_policy.RandomTFPolicy(\n",
    "    action_spec=env.action_spec(),\n",
    "    time_step_spec=env.time_step_spec(),)\n",
    "replay_buffer = []\n",
    "avg_return = tf_metrics.AverageReturnMetric()\n",
    "n_episodes = tf_metrics.NumberOfEpisodes()\n",
    "n_steps = tf_metrics.EnvironmentSteps()\n",
    "observers = [replay_buffer.append, avg_return, n_episodes, n_steps]\n",
    "driver, final_step, policy_state = evaluate_policy(rand_policy, observers)\n",
    "print('Number of Steps: ', n_steps.result().numpy())\n",
    "print('Number of Episodes: ', n_episodes.result().numpy())\n",
    "print('Average Return: ', avg_return.result().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Player: [6, 7] -> 13. Dealer: [10, 10] -> 20. Reward: -1.\n",
      "Number of Steps:  8\n",
      "Number of Episodes:  6\n",
      "Average Return:  -0.33333334\n"
     ]
    }
   ],
   "source": [
    "final_step, policy_state = driver.run(final_step, policy_state, num_episodes=1)\n",
    "print('Number of Steps: ', n_steps.result().numpy())\n",
    "print('Number of Episodes: ', n_episodes.result().numpy())\n",
    "print('Average Return: ', avg_return.result().numpy())\n"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
