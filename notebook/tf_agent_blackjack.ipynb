{"cells":[{"cell_type":"markdown","source":[" # TF Agent で Blackjack 遊ぶ\n","\n"," ブラックジャックを参考に、次のような\n"," ルールとする。\n","\n"," - カードの値は、 1〜11の間にランダムに決まる（エース考慮などはしない）\n"," - 最初にカードをプレイヤーに2枚、ディーラーに1枚\n"," - プレイヤーが何枚でもカードを引ける(hit)が、合計が21超えたら即負け。ゲーム終了\n"," - プレイヤーがカードを引くのを止めたら(stick)、ディーラーがカードを引く番になる\n"," - ディーラーは、カードの合計が17に達するまでカードを強制的に引く\n"," - ディーラーのカードの合計が21超えたら、プレイヤーの勝ち。ゲーム終了\n"," - ディーラーとプレイヤーとでカードの合計を比較して、高いほうが勝ち。ゲーム終了"],"metadata":{}},{"source":["# !which python\n","# !sudo apt install -y cuda-cublas-10-0  cuda-cusolver-10-0 cuda-cudart-10-0 cuda-cusparse-10-0\n","# !conda install -y -c anaconda cudatoolkit\n","# !pip install tf-nightly-gpu tf-agents-nightly 'gym==0.10.11'\n","from __future__ import absolute_import, division, print_function\n","import matplotlib.pyplot as plt\n","from tf_agents.drivers import dynamic_step_driver\n","from tf_agents.replay_buffers import tf_uniform_replay_buffer\n","from tf_agents.agents.dqn import q_network\n","from tf_agents.agents.dqn import dqn_agent\n","from tf_agents.drivers import dynamic_episode_driver\n","from tf_agents.metrics import tf_metrics\n","from tf_agents.policies import random_tf_policy\n","from tf_agents.environments import utils\n","import numpy as np\n","import tensorflow as tf\n","from tf_agents.environments import py_environment\n","from tf_agents.environments import tf_environment\n","from tf_agents.environments import tf_py_environment\n","from tf_agents.environments import suite_gym\n","from tf_agents.environments import time_step\n","from tf_agents.specs import array_spec\n","\n","tf.compat.v1.enable_v2_behavior()\n","assert tf.executing_eagerly()\n","# tf.enable_eager_execution()\n","\n","DEBUG = False\n","num_eval_episodes = 5  # @param\n","\n","\n","def plog(msg, *args):\n","    if DEBUG:\n","        print(msg.format(*args))\n","\n","\n","class BlackJackEnv(py_environment.PyEnvironment):\n","    # Simplified Blackjack\n","    ACT_HIT = 0\n","    ACT_STICK = 1\n","    LIMIT_SCORE = 21\n","\n","    def __init__(self, state_len=1):\n","        self._batch_size = 1  # batch_size\n","        self._state_len = state_len\n","        self._action_spec = array_spec.BoundedArraySpec(\n","            shape=(), dtype=np.int32, name='action',\n","            minimum=self.ACT_HIT, maximum=self.ACT_STICK,\n","        )\n","        self._observation_spec = array_spec.BoundedArraySpec(\n","            shape=(self._state_len,), dtype=np.int32, minimum=0,\n","            name='observation'\n","        )\n","        self.reset()\n","        return\n","\n","    def _state(self):\n","        if self._state_len == 1:\n","            return self._state_player_sum()\n","        return self._state_last_cards()\n","\n","    def _state_player_sum(self):\n","        # Return the player current score\n","        state = [np.sum(self._player_cards)]\n","        return np.array(state, dtype=np.int32)\n","\n","    def _state_last_cards(self):\n","        # Full state includes 1st card of the dealer and all cards of player,\n","        # but this return only the last _state_len cards.\n","        state = [self._dealer_cards[0]] + self._player_cards\n","        if len(state) < self._state_len:\n","            state = np.pad(state, (0, self._state_len-len(state)),\n","                           'constant', constant_values=(0))\n","        return np.array(state[-self._state_len:], dtype=np.int32)\n","\n","    def action_spec(self):\n","        return self._action_spec\n","\n","    def observation_spec(self):\n","        return self._observation_spec\n","\n","    def __reset(self):\n","        self._player_cards = [self._new_card(), self._new_card()]\n","        self._dealer_cards = [self._new_card()]\n","        self._episode_ended = False\n","\n","    def _reset(self):\n","        self.__reset()\n","        # self._current_time_step = time_step.restart(self._state())\n","        # return self._current_time_step\n","        return time_step.restart(self._state())\n","\n","    def _new_card(self):\n","        # Simplified Blackjack rule\n","        new_card = np.random.randint(1, 11+1)\n","        return new_card\n","\n","    def _dealer_hit(self):\n","        while np.sum(self._dealer_cards) < 17:\n","            self._dealer_cards.append(self._new_card())\n","        return np.sum(self._dealer_cards)\n","\n","    def _player_score(self):\n","        return np.sum(self._player_cards)\n","\n","    def _terminate(self, reward):\n","        plog(\n","            \"Player: {} -> {}. Dealer: {} -> {}. Reward: {}.\",\n","            self._player_cards, np.sum(self._player_cards),\n","            self._dealer_cards, np.sum(self._dealer_cards),\n","            reward)\n","        self._episode_ended = True\n","        return time_step.termination(self._state(), reward)\n","\n","    def _step(self, action):\n","        if self._episode_ended:\n","            return self.reset()  # don't forget to `return`\n","\n","        if action == self.ACT_HIT:\n","            self._player_cards.append(self._new_card())\n","            if self._player_score() > self.LIMIT_SCORE:  # the player goes bust\n","                return self._terminate(-1)\n","\n","            return time_step.transition(self._state(), reward=0, discount=1)\n","\n","        # Afteward action == self.ACT_STICK\n","        dealer_score = self._dealer_hit()\n","        player_score = self._player_score()\n","        if dealer_score > self.LIMIT_SCORE or dealer_score < player_score:\n","            reward = 1\n","        elif dealer_score == player_score:\n","            reward = 0\n","        else:\n","            reward = -1\n","        return self._terminate(reward)\n","\n","    @classmethod\n","    def tf_env(cls):\n","        return tf_py_environment.TFPyEnvironment(cls())\n","\n","\n","def print_spec(env):\n","    act_spec, ts_spec = env.action_spec(), env.time_step_spec()\n","    for x in (act_spec, ts_spec.observation, ts_spec.step_type,\n","              ts_spec.discount, ts_spec.reward):\n","        print(x)\n","    return\n","\n","\n","# TODO: validate_py_environment should check for a reset()\n","utils.validate_py_environment(BlackJackEnv())\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[" ## for loop でランダムに遊ぶ場合\n","\n"," プレイヤーがカードを最大 `n_max_cards` 枚引く。\n"," 平均的に見たら負けています。\n",""],"metadata":{}},{"source":["\n","\n","def play_blackjack(env, n_max_cards=1):\n","    ts = env.reset()\n","    gain = ts.reward\n","    cards = []\n","    for _ in range(np.random.randint(n_max_cards+1)):\n","        if ts.is_last():\n","            break\n","        ts = env.step(tf.constant([BlackJackEnv.ACT_HIT]))\n","        cards += [ts.observation[0][0].numpy()]\n","        gain += ts.reward\n","\n","    if not ts.is_last():\n","        ts = env.step(tf.constant([BlackJackEnv.ACT_STICK]))\n","        gain += ts.reward\n","    gain = gain.numpy()[0]\n","    return cards, gain\n","\n","\n","env = BlackJackEnv.tf_env()\n","gains = []\n","for _ in range(num_eval_episodes):\n","    _, gain = play_blackjack(env, 2)\n","    gains.append(gain)\n","mean_score1 = np.mean(gains)\n","mean_score1\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[" ## RandomTFPolicyでランダムに遊ぶ\n","\n"," Agentに見せる環境の情報 (state) は2パターン\n"," - プレイヤーのカードの合計値\n"," - ディーラーの最初のカードと、プレイヤーが引いた全てのカード"],"metadata":{}},{"source":["\n","\n","def evaluate_policy(\n","        policy,\n","        num_episodes=num_eval_episodes,\n","        eval_env=BlackJackEnv.tf_env(),\n","):\n","    avg_return = tf_metrics.AverageReturnMetric()\n","    # n_episodes = tf_metrics.NumberOfEpisodes()\n","    # n_steps = tf_metrics.EnvironmentSteps()\n","    observers = [avg_return,\n","                 #  n_episodes, n_steps\n","                 ]\n","    driver = dynamic_episode_driver.DynamicEpisodeDriver(\n","        eval_env, policy, observers, num_episodes)\n","    final_step, policy_state = driver.run(num_episodes=num_episodes)\n","    # print('Number of Steps: ', n_steps.result().numpy())\n","    # print('Number of Episodes: ', n_episodes.result().numpy())\n","    # print('Average Return: ', avg_return.result().numpy())\n","    return driver, final_step, policy_state, avg_return.result().numpy()\n","\n","\n","DEBUG = False\n","env = BlackJackEnv.tf_env()\n","rand_policy = random_tf_policy.RandomTFPolicy(\n","    action_spec=env.action_spec(),\n","    time_step_spec=env.time_step_spec(),)"],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["avg_returns = []\n","for n_episodes in range(100, 210, 10):\n","    _, _, _, avg_return = evaluate_policy(rand_policy, num_episodes=n_episodes)\n","    avg_returns.append(avg_return)\n","\n","\n","\n","\n","plt.plot(avg_returns)\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"cell_type":"markdown","source":[" ## DQNで強化学習"],"metadata":{}},{"source":["\n","\n","class DqnAgent:\n","    def __init__(self, env):\n","        # Agent初期化\n","        self.env = env\n","        q_net = q_network.QNetwork(\n","            env.observation_spec(),\n","            env.action_spec(),\n","            fc_layer_params=fc_layer_params,\n","        )\n","\n","        adam = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate)\n","\n","        train_step_counter = tf.compat.v2.Variable(0)\n","\n","        self.agent = dqn_agent.DqnAgent(\n","            env.time_step_spec(),\n","            env.action_spec(),\n","            q_network=q_net,\n","            optimizer=adam,\n","            td_errors_loss_fn=dqn_agent.element_wise_squared_loss,\n","            train_step_counter=train_step_counter,\n","        )\n","        self.agent.initialize()\n","        self._create_replay_buffer()\n","\n","    def _create_replay_buffer(self):\n","        # Replay Bufferの初期化。初期データ収集\n","        self.replay_buffer = buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n","            data_spec=self.agent.collect_data_spec,\n","            batch_size=self.env.batch_size,  # actually 1, env isn't batched\n","            max_length=replay_buffer_capacity\n","        )\n","        print(buffer.capacity.numpy(), buffer._batch_size)\n","        print(buffer.data_spec)\n","        self._collect_data(rand_policy, initial_collect_steps)\n","        dataset = buffer.as_dataset(\n","            num_parallel_calls=3, num_steps=2,\n","            sample_batch_size=batch_size,\n","        ).prefetch(batch_size)\n","        self.data_iterator = iter(dataset)\n","\n","    def _collect_data(self, policy, n_steps):\n","        # Replay Bufferへのデータ追加\n","        dynamic_step_driver.DynamicStepDriver(\n","            self.env, policy, [self.replay_buffer.add_batch], n_steps\n","        ).run()\n","        return\n","\n","    def train(self, num_iterations):\n","        _, _, _, avg_return = evaluate_policy(\n","            self.agent.policy, num_eval_episodes)\n","        avg_returns = [avg_return]\n","        for step in range(1, 1 + num_iterations):\n","            self._collect_data(self.agent.collect_policy,\n","                               collect_steps_per_iteration)\n","            experience, _ = next(self.data_iterator)\n","            train_loss = self.agent.train(experience)\n","            self._print_log(step, train_loss.loss, avg_returns)\n","        return avg_returns\n","\n","    def _print_log(self, step, loss, avg_returns):\n","        if step % log_interval == 0:\n","            print(f'Step {step: >3}. Loss {loss}.')\n","        if step % eval_interval == 0:\n","            _, _, _, avg_return = evaluate_policy(\n","                self.agent.policy, num_eval_episodes)\n","            print(f'Step {step: >3}. AvgReturn {avg_return}.')\n","            avg_returns.append(avg_return)\n","\n","def plot(avg_returns, num_iterations, eval_interval):\n","    steps = range(0, num_iterations + 1, eval_interval)\n","    plt.ylabel('Average Return')\n","    plt.xlabel('Step')\n","    plt.plot(steps, avg_returns)\n","    # plt.ylim(top=210)\n","\n","DEBUG = False\n","learning_rate = 1e-3  # @param\n","batch_size = 64*4  # @param\n","collect_steps_per_iteration = 100  # @param\n","initial_collect_steps = collect_steps_per_iteration*1  # @param\n","num_eval_episodes = collect_steps_per_iteration  # @param\n","replay_buffer_capacity = collect_steps_per_iteration*10  # @param\n","fc_layer_params = (100, 100, )  # @param\n","log_interval = 10  # @param\n","eval_interval = log_interval*5  # @param\n","num_iterations = eval_interval*20  # @param\n","assert eval_interval % log_interval == 0\n","\n","# Set a `bad` _state_len and see that it can't learn\n","dqn = DqnAgent(BlackJackEnv.tf_env())\n","avg_returns = dqn.train(num_iterations)\n","\n","plot(avg_returns, num_iterations, eval_interval)\n","# assert flat_action_spec[0].shape.ndims <= 1"],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0},{"source":["# (1,).ndim\n",""],"cell_type":"code","outputs":[],"metadata":{},"execution_count":0}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3}},"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3}}