{"cells":[{"cell_type":"markdown","metadata":{},"source":[" # TF Agents の環境"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["# !pip install tf-nightly tf-agents-nightly 'gym==0.10.11'\n","from __future__ import absolute_import, division, print_function\n","from tf_agents.environments import utils\n","import numpy as np\n","import tensorflow as tf\n","from tf_agents.environments import py_environment\n","from tf_agents.environments import tf_environment\n","from tf_agents.environments import tf_py_environment\n","from tf_agents.environments import suite_gym\n","from tf_agents.environments import time_step\n","from tf_agents.specs import array_spec\n","\n","tf.compat.v1.enable_v2_behavior()\n",""]},{"cell_type":"markdown","metadata":{},"source":[" ## Python環境"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"BoundedArraySpec(shape=(), dtype=dtype('int64'), name=None, minimum=0, maximum=1)\nBoundedArraySpec(shape=(4,), dtype=dtype('float32'), name=None, minimum=[-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], maximum=[4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38])\nArraySpec(shape=(), dtype=dtype('int32'), name='step_type')\nBoundedArraySpec(shape=(), dtype=dtype('float32'), name='discount', minimum=0.0, maximum=1.0)\nArraySpec(shape=(), dtype=dtype('float32'), name='reward')\n.................\nTotal gain: 17.0\n"}],"source":["# Python Environment\n","env = suite_gym.load(\"CartPole-v0\")\n","act_spec, ts_spec = env.action_spec(), env.time_step_spec()\n","for x in (act_spec, ts_spec.observation, ts_spec.step_type, ts_spec.discount, ts_spec.reward):\n","    print(x)\n","\n","ts = env.reset()\n","gain = 0\n","while not ts.is_last():\n","    action = np.random.randint(2)\n","    ts = env.step(action)\n","    print('.', end='')\n","    gain += ts.reward\n","print(\"\\nTotal gain:\", gain)\n","\n",""]},{"cell_type":"markdown","metadata":{},"source":[" ## Python環境をTF環境でラッピング"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":"WARNING: Logging before flag parsing goes to stderr.\nW0409 10:25:46.568527 4516206016 deprecation.py:237] From <ipython-input-3-0cba6d369f85>:10: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n\nW0409 10:25:46.570624 4516206016 backprop.py:818] The dtype of the watched tensor must be floating (e.g. tf.float32), got tf.int32\nW0409 10:25:46.574940 4516206016 backprop.py:818] The dtype of the watched tensor must be floating (e.g. tf.float32), got tf.int32\nW0409 10:25:46.578200 4516206016 backprop.py:818] The dtype of the watched tensor must be floating (e.g. tf.float32), got tf.int32\nW0409 10:25:46.581434 4516206016 backprop.py:818] The dtype of the watched tensor must be floating (e.g. tf.float32), got tf.int32\nW0409 10:25:46.584511 4516206016 backprop.py:818] The dtype of the watched tensor must be floating (e.g. tf.float32), got tf.int32\n"},{"name":"stdout","output_type":"stream","text":"BoundedTensorSpec(shape=(), dtype=tf.int64, name=None, minimum=array(0), maximum=array(1))\nBoundedTensorSpec(shape=(4,), dtype=tf.float32, name=None, minimum=array([-4.8000002e+00, -3.4028235e+38, -4.1887903e-01, -3.4028235e+38],\n      dtype=float32), maximum=array([4.8000002e+00, 3.4028235e+38, 4.1887903e-01, 3.4028235e+38],\n      dtype=float32))\nTensorSpec(shape=(), dtype=tf.int32, name='step_type')\nBoundedTensorSpec(shape=(), dtype=tf.float32, name='discount', minimum=array(0., dtype=float32), maximum=array(1., dtype=float32))\nTensorSpec(shape=(), dtype=tf.float32, name='reward')\n................\n tf.Tensor([16.], shape=(1,), dtype=float32)\n\nTotal gain: 16.0\n"}],"source":["# Wrapping a PyEnv in TF\n","env = suite_gym.load(\"CartPole-v0\")\n","env = tf_py_environment.TFPyEnvironment(env)\n","act_spec, ts_spec = env.action_spec(), env.time_step_spec()\n","for x in (act_spec, ts_spec.observation, ts_spec.step_type, ts_spec.discount, ts_spec.reward):\n","    print(x)\n","\n","ts = env.reset()\n","gain = 0\n","while not ts.is_last():\n","    action = tf.random_uniform([1], 0, 2, dtype=tf.int32)\n","    ts = env.step(action)\n","    print('.', end='')\n","    gain += ts.reward\n","print(\"\\n\", gain)\n","print(\"\\nTotal gain:\", gain.numpy()[0])\n",""]},{"cell_type":"markdown","metadata":{},"source":[" ## Python環境を新規作成"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"New card: 6, Sum: 6\nNew card: 4, Sum: 10\nNew card: 10, Sum: 20\nEnd of game, rewarded 20\n\nTotal gain: 20.0\n"}],"source":["\n","\n","class BlackJackEnv(py_environment.PyEnvironment):\n","    ACTION_GET_NEW_CARD = 0\n","    ACTION_END_GAME = 1\n","    LIMIT_STATE = 21\n","\n","    def __init__(self):\n","        self._action_spec = array_spec.BoundedArraySpec(\n","            shape=(), dtype=np.int32, minimum=0, maximum=1,\n","            name='action'\n","        )\n","        self._observation_spec = array_spec.BoundedArraySpec(\n","            shape=(1,), dtype=np.int32, minimum=0,\n","            name='observation'\n","        )\n","        self._state = 0\n","        self._episode_ended = False\n","        return\n","\n","    def action_spec(self):\n","        return self._action_spec\n","\n","    def observation_spec(self):\n","        return self._observation_spec\n","\n","    def _reset(self):\n","        self._state = 0\n","        self._episode_ended = False\n","        return time_step.restart(np.array([self._state], dtype=np.int32))\n","\n","    def _step(self, action):\n","        if self._episode_ended:\n","            return self.reset()\n","\n","        if action == self.ACTION_END_GAME:\n","            self._episode_ended = True\n","        elif action == self.ACTION_GET_NEW_CARD:\n","            new_card = np.random.randint(1, 11)\n","            self._state += new_card\n","            print(\"New card: {}, Sum: {}\".format(new_card, self._state))\n","        else:\n","            raise ValueError(\"`action` should be {} or {}\".format(\n","                self.ACTION_GET_NEW_CARD, self.ACTION_END_GAME\n","            ))\n","\n","        if self._episode_ended or self._state >= self.LIMIT_STATE:\n","            reward = self._state if self._state <= self.LIMIT_STATE else -99\n","            print(\"End of game, rewarded\", reward)\n","            return time_step.termination(\n","                np.array([self._state], dtype=np.int32), reward)\n","\n","        return time_step.transition(\n","            np.array([self._state], dtype=np.int32),\n","            reward=0.0,\n","            discount=1.0)\n","\n","\n","env = BlackJackEnv()\n","# utils.validate_py_environment(env)\n","\n","\n","# act_spec, ts_spec = env.action_spec(), env.time_step_spec()\n","# for x in (act_spec, ts_spec.observation,\n","#           ts_spec.step_type, ts_spec.discount, ts_spec.reward):\n","#     print(x)\n","\n","ts = env.reset()\n","gain = ts.reward\n","for _ in range(3):\n","    if ts.is_last(): break\n","    action = np.random.randint(2)\n","    action = BlackJackEnv.ACTION_GET_NEW_CARD\n","    ts = env.step(action)\n","    # print(ts)\n","    gain += ts.reward\n","\n","if not ts.is_last():\n","    action = BlackJackEnv.ACTION_END_GAME\n","    ts = env.step(action)\n","    # print(ts)\n","    gain += ts.reward\n","print(\"\\nTotal gain:\", gain)\n",""]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":""}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3}},"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3}}